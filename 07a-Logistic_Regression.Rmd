---
title: "07a- Logistic Regression"
output:
  html_document:
    toc: yes
  html_notebook: default
  pdf_document:
    toc: yes
---

```{r eval=TRUE, echo=F, message=FALSE, warning=FALSE}
library(knitr)
options(digits = 3)
opts_chunk$set(tidy.opts=list(width.cutoff=50), tidy=TRUE, rownames.print = FALSE, rows.print = 10, message = FALSE, warning = FALSE)
```

# Logistic regression

## Motivation and intuition

In the last chapter we saw how to predict continuous outcomes (sales, height, etc.) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. To further expand on this, consider the following simulated data. $Y$ is the outcome we want to explain and $\mathbf{X}$ is our sole predictor. Looking at the scatter plot we can already see that $\mathbf{X}$ probably has an effect on $Y$. But how can we model it correctly?

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="Simulated binary outcome data"}
library(ggplot2)
library(gridExtra)

set.seed(1776) # AMERICA THE BEAUTIFUL

beta.true <- as.matrix(c(0.6))
N <- 100
K <- nrow(beta.true)
X <- matrix(runif(K * N, -8, 8), ncol = K)
probs <- 1/(1 + exp(-X %*% beta.true))

Y <- rbinom(n = N, size = 1, prob = probs)
  
lin.mod <- lm(Y ~ X)$coefficients

lin.mod.pred <- cbind(1,X) %*% lin.mod

ggplot(data = data.frame(Y = Y, X = X), aes(x = X, y = Y)) +
  geom_point() + 
  theme_bw()
```

Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a logistic regression model. As you can see, the linear probability model produces probabilities that are above 1 and below 0, which are not valid probabilites, while the logistic model stays between 0 and 1. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="The same binary data explained by two models; A linear probabilty model (on the left) and a logistic regression model (on the right)"}
plot.bad <- ggplot(data = as.data.frame(cbind(Y, X)), aes(x = X, y = Y)) +
  geom_point() + 
  geom_line(data = data.frame(pred = lin.mod.pred[order(X)], X = X[order(X)]), aes(y = pred), lwd = 1.01, color = "firebrick") + 
  theme_bw() +
  ylim(-0.2, 1.2) 

plot.cor <- ggplot(data = as.data.frame(cbind(Y, X)), aes(x = X, y = Y)) +
  geom_point() + 
  geom_line(data = data.frame(probs = probs[order(X)], X = X[order(X)]), aes(y = probs), lwd = 1.01, color = "firebrick") + 
  theme_bw() +
  ylim(-0.2, 1.2) +
  theme(axis.title.y = element_blank())

grid.arrange(plot.bad, plot.cor, ncol = 2)
```

A key insight at this point is that the connection between $\mathbf{X}$ and $Y$ is __non-linear__ in the logistic regression model. As we can see in the plot, the probability of success is most strongly affected by $\mathbf{X}$ around values of 0, while higher and lower values have a smaller marginal effect. This obviously also has consequences for the interpretation of the coefficients later on.  

## Technical details of the model

As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form:

$$
f(\mathbf{X}) = \frac{1}{1 + e^{-\mathbf{X}}}
$$
This function transfroms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1. 

```{r, echo = FALSE}
library(latex2exp)
x <- seq(-10, 10, length.out = 1000)
fx <- 1/(1+exp(-x))
df <- data.frame(x = x, fx = fx)
ggplot(df, aes(x = x, y = fx)) + 
  geom_line()+
  labs(y = TeX("$\\frac{1}{1+e^{-\\mathbf{X}}}$"), x = TeX("$\\mathbf{X}$"))+
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```



The $\mathbf{X}$ in the function above is then replaced by our familiar linear specification, i.e.

$$
\mathbf{X} = \beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i}\\
f(\mathbf{X}) = P(y_i = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i})}}
$$

## Estimation in R

In order to perform a logistic regression in R the ```glm(Y~X, family = binomial(link = "logit"))``` function can be used (you can replace "logit" with "probit" for a probit regression). The coefficients of the model give the change in the [log odds](https://en.wikipedia.org/wiki/Odds#Statistical_usage) of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being $1$. 

We can predict the probability given an observation using the ```predict(my_logit, newdata = ..., type = "response")``` function. Replace ```...``` with the observed $x_{i,j}$ values for which you would like to predict the outcome variable.

In order to get a rough idea about the magnitude of the effects we can calculate the partial effects at the mean of the data (that is the effect for the average observation). Alternatively, we can calculate the mean of the effects (that is the average of the individual effects). Both can be done with the ```logitmfx(...)``` function from the ```mfx``` package. If we set ```logitmfx(my_logit, data = my_data, atmean = FALSE)``` we calculate the latter. Setting ```atmean = TRUE``` will calculate the former. However, in general we are most interested in the sign and significance of the coefficient. In order to measure the quality of the model overall we can use the Akaike information criterion ("AIC"). It is reported with the summary output for logit models. The value of the AIC has no meaning by itself. However, it can be used to compare and select models. The model with the lowest AIC value is the one that should be chosen. Note that the AIC does not indicate how well the model fits the data, but is merely used to compare models. Therefore, we need a measure comparable to the $R^2$ for linear regressions. In fact multiple "Pseudo $R^2$s" have been developed. There are multiple packages that return the $R^2$ given a logit model (see ```rcompanion``` or ```pscl```). The calculation by hand is also fairly simple. This is left as an exercise for the reader.   

## Example

In this example we are going to analyse the probability that somebody cheats on their spouse. As explanatory variables sex, age, religiousness (1 = anti, 5 = very), self rating of marriage (1 = very unhappy, 5 = very happy) and years married are used. We observe that one's sex does not have a significant impact on the probability of cheating. The older someone gets the less likely it is for them to cheat. More religious people are also less likely to cheat. Unsurprisingly being happier in one's marriage also decreases the likelihood of cheating. On the other hand, the longer someone is married the more likely they are to have cheated. Education does not have a significant impact. If we compare the first model with one that does not include religiousness we notice that the AIC is lower in the former model. Thus, between the two we would choose the first. Next we calculate the average partial effect using ```logitmfx(...)```. This shows that, on average, giving ones marriage the highest rating decreases the probability of cheating by $\sim 25\%$. Finally, we look at making predictions with our model using ```predict(...)```. The prediction indicates that a 30 year old male who has been married for two years, has 16 years of education, highly rates his marriage and is not very religious has a probability of cheating of $\sim 17\%$ (Try predicting your own likelihood of cheating).


```{r}
library(Ecdat)
data("Fair") # needs library(Ecdat)
head(Fair)

# Recoding number of affairs to binary variable
Fair$nbaffairs[Fair$nbaffairs>0] <- 1
# The model
my_logit <- glm(nbaffairs~sex+age+factor(religious)+factor(rate)+ym+education, data = Fair, family = binomial(link = "logit"))
summary(my_logit)

# Take "religious" out. Compare AIC!
my_logit2 <- glm(nbaffairs~sex+age+factor(rate)+ym+education, data = Fair, family = binomial(link = "logit"))
summary(my_logit2)

library(mfx)
# Average partial effect
logitmfx(my_logit, data = Fair, atmean = FALSE)

# Prediction for one observation
predict(my_logit, newdata = data.frame(sex = "male", age = 30, ym = 2, religious = 2, education = 16, rate = 4), type = "response")
```

```{r echo = FALSE}
logisticPseudoR2s <- function(LogModel) {
  dev <- LogModel$deviance 
  nullDev <- LogModel$null.deviance 
  modelN <- length(LogModel$fitted.values)
  R.l <-  1 -  dev / nullDev
  R.cs <- 1- exp ( -(nullDev - dev) / modelN)
  R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))
  cat("Pseudo R^2 for logistic regression\n")
  cat("Hosmer and Lemeshow R^2  ", round(R.l, 3), "\n")
  cat("Cox and Snell R^2        ", round(R.cs, 3), "\n")
  cat("Nagelkerke R^2           ", round(R.n, 3),    "\n")
}

```

Below three versions of pseudo $R^2$s are reported. They indicate that our model does not describe our data well. So do not worry too much if the model indicates your partner has a high probability of cheating! As an exercise try to write a function that will return the following output:

```{r}
logisticPseudoR2s(my_logit) # try to write this function!!
```

## Perfect Prediction Logit

Perfect prediction occurs whenever a linear function of $X$ can perfectly separate the $1$s from the $0$s in the dependent variable. This is problematic when estimating a logit model as it will result in biased estimators (also check to p-values in the example!). R will return the following message if this occurs:

```glm.fit: fitted probabilities numerically 0 or 1 occurred```

Given this error, one should not use the output of the ```glm(...)``` function for the analysis. There are [various ways](https://stats.stackexchange.com/a/68917) to deal with this problem, one of which is to use Firth's bias-Reduced penalized-likelihood logistic regression with the ```logistf(Y~X)``` function in the ```logistf``` package.  

## Example

In this example data $Y = 0$ if $x_1 <0$ and $Y=1$ if $x_1>0$ and we thus have perfect prediction. As we can see the output of the regular logit model is not interpretable. The standard errors are huge compared to the coefficients and thus the p-values are $1$ despite $x_1$ being a predictor of $Y$. Thus we turn to the penalized-likelihood version. This model correctly indicates that $x_1$ is in fact a predictor for $Y$ as the coefficient is significant.  

```{r}
Y <- c(0,0,0,0,1,1,1,1)
X <- cbind(c(-1,-2,-3,-3,5,6,10,11),c(3,2,-1,-1,2,4,1,0))

# Perfect prediction with regular logit
summary(glm(Y~X, family=binomial(link="logit")))

library(logistf)
# Perfect prediction with penalized-likelihood logit
summary(logistf(Y~X))
```

## Estimation of the parameters $\beta_i$

Let's return to the model to figure out how the parameters $\beta_i$ are estimated. In the data we observe the $y_i$, the outcome variable that is either $0$ or $1$, and the $x_{i,j}$, the predictors for each observation. In addition, by using the logit model we have assumed the following functional form. So far we have looked at estimating the $\beta_i$ with R but have not discussed how that works.

$$
P(y_i = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i})}}
$$

In contrast to linear models (e.g. OLS) we cannot rely on linear algebra to solve for $\beta_i$ since the function is now non-linear. Therefor, we turn to a methodology called maximum likelihood estimation. Basically, we try out different $\beta_i$ and choose the combination that best fits the observed data. In other words: choose the combination of $\beta_i$ that maximize the likelihood of observing the given dataset. 

### Example

Consider an experiment in which we want to find out whether a person will listen to the full song or skip it. Further assume that the genre of the song is the only determining factor of wheter somebody is likely to skip it or not. Each individual has rated the genre on a scale from 1 (worst) to 10 (best). Our model looks as follows:

$$ 
P(\text{skipped}_i) = {1 \over 1 + e^{-(\beta_0 + \beta_1 \text{rating}_i)}}
$$

The probability that individual $i$ will skip a song is the logisitc function with $X = \beta_0 + \beta_1 * \text{rating}_i$, where $\text{rating}_i$ is individual $i$'s rating of the genre of the song. Since we assume independence of individuals we can write the joint likelihood as

$$
\prod_{i=1}^{N} \left({1 \over 1 + e^{-(\beta_0 + \beta_1 \text{rating}_i)}}\right)^{y_i} + \left(1- {1 \over 1 + e^{-(\beta_0 + \beta_1 \text{rating}_i)}}\right)^{(1-y_i)}
$$
Notice that $y_i$ is either equal to one or to zero. So for each individual only one of the two parts is not equal to one (recall that any real number to the power of 0 is equal to 1). The the part left of the plus sign is "looking at" individuals who skipped the song given their rating and the right part is looking at individuals who did not skip the song given their rating. For the former we want the predicted probability of skipping to be as high as possible. For the latter we want the predicted probability of *not* skipping to be as high as possible and thus write $1 - {1 \over 1+ e^{-(\beta_0 + \beta_1 \text{rating}_i)}}$. This is convenient since we can now maximize a single function. Another way to simplify the calculation and make it computationally feasible is taking the logarithm. This will ensure that extremely small probabilities can still be processed by the computer ([see this illustration](#sum-of-ln-vs-product)).    


```{r likesSongExample}
set.seed(1234)
ratingGenre <- floor(runif(10000, 1, 10))
pSkip <- 1/(1+exp(-(1 - 0.75 * ratingGenre)))
skipped <- sapply(pSkip, function(p) sample(c(1, 0), size = 1, prob = c(p, 1-p)))

x <- cbind(1, ratingGenre)
mlEstimate <- function(params){
  pred <- 1/(1+exp(-(x %*% params)))
  loglik <- skipped * log(pred) + (1-skipped) * log(1 - pred)
  sum(loglik)
}
a <- seq(0.5, 1.5, length.out = 100)
b <- seq(-1, 0, length.out = 100)
params <- expand.grid(a,b)
j <- 1:10000
logliks <- sapply(j,function(x) mlEstimate(t(params[x,])))
maxLik <- which(logliks == max(logliks))

params[maxLik, ]

summary(glm(skipped ~ ratingGenre, family = binomial(link = 'logit')))

```

#### Sum of LN vs Product

To illustrate why the natural log (ln) is important for computations with numbers close to zero (such as joint probabilities), we create a vector of fictitious probabilities and multiply them with each other. As we can see, already a small number of values close to 0 will lead to their product being erroneously interpreted as being 0. If we are only interested in comparing magnitude (as in the case of likelihoods) we can safely apply the ln, since it is a monotonically increasing function and will thus not change the ranking. 
```{r}
probs <- c(0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001,
           0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001)
prod(probs)

lProbs <- log(probs) # this is the ln
sum(lProbs)

probs2 <- rbind(probs, probs, probs, probs, probs,
                probs, probs, probs, probs, probs) # now 10 times as many values
lProbs2 <- log(probs2)
sum(lProbs2)
```

