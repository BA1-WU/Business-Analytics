---
title: "07a- Logistic Regression"
output:
  html_document:
    toc: yes
  html_notebook: default
  pdf_document:
    toc: yes
---

```{r message=FALSE, warning=FALSE, echo=F, eval=TRUE,paged.print = FALSE}
options(digits = 3)
```

# Logistic regression

## Motivation and intuition

In the last chapter we saw how to predict continuous outcomes (sales, height, etc.) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. To further expand on this, consider the following simulated data. $Y$ is the outcome we want to explain and $\mathbb{x}$ is our sole predictor. Looking at the scatter plot we can already see that $\mathbb{x}$ probably has an effect on $Y$. But how can we model it correctly?

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="Simulated binary outcome data"}
library(ggplot2)
library(gridExtra)

set.seed(1776) # AMERICA THE BEAUTIFUL

beta.true <- as.matrix(c(0.6))
N <- 100
K <- nrow(beta.true)
X <- matrix(runif(K * N, -8, 8), ncol = K)
probs <- 1/(1 + exp(-X %*% beta.true))

Y <- rbinom(n = N, size = 1, prob = probs)
  
lin.mod <- lm(Y ~ X)$coefficients

lin.mod.pred <- cbind(1,X) %*% lin.mod

ggplot(data = data.frame(Y = Y, X = X), aes(x = X, y = Y)) +
  geom_point() + 
  theme_bw()
```

Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a logistic regression model. As you can see, the linear probability model produces probabilities that are above 1 and below 0, which are not valid probabilites, while the logistic model stays between 0 and 1. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.cap="The same binary data explained by two models; A linear probabilty model (on the left) and a logistic regression model (on the right)"}
plot.bad <- ggplot(data = as.data.frame(cbind(Y, X)), aes(x = X, y = Y)) +
  geom_point() + 
  geom_line(data = data.frame(pred = lin.mod.pred[order(X)], X = X[order(X)]), aes(y = pred), lwd = 1.01, color = "firebrick") + 
  theme_bw() +
  ylim(-0.2, 1.2) 

plot.cor <- ggplot(data = as.data.frame(cbind(Y, X)), aes(x = X, y = Y)) +
  geom_point() + 
  geom_line(data = data.frame(probs = probs[order(X)], X = X[order(X)]), aes(y = probs), lwd = 1.01, color = "firebrick") + 
  theme_bw() +
  ylim(-0.2, 1.2) +
  theme(axis.title.y = element_blank())

grid.arrange(plot.bad, plot.cor, ncol = 2)
```

A key insight at this point is that the connection between $\mathbb{x}$ and $Y$ is __non-linear__ in the logistic regression model. As we can see in the plot, the probability of success is most strongly affected by X around values of 0, while higher and lower values have a smaller marginal effect. This obviously also has consequences for the interpretation of the coefficients later on.  

## Technical details of the model

As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form:

$$
f(x) = \frac{1}{1 + e^{-\mathbb{x}}}
$$
This function transfroms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1. 

```{r}
library(latex2exp)
x <- seq(-10, 10, length.out = 1000)
fx <- 1/(1+exp(-x))
df <- data.frame(x = x, fx = fx)
ggplot(df, aes(x = x, y = fx)) + 
  geom_line()+
  labs(y = TeX("$\\frac{1}{1+e^{-x}}}$"))+
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```



The $\mathbb{x}$ in the function above is then replaced by our familiar linear specification, i.e.

$$
P(y_i = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i})}}
$$

## Estimation in R

In order to perform a logistic regression in R the ```glm(Y~X, family = binomial(link = "logit"))``` function can be used (you can replace "logit" with "probit" for a probit regression). The coefficients of the model give the change in the log odds of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being $1$ (i.e. uno). 

We can predict the probability given an observation using the ```predict(my_logit, newdata = ..., type = "response")``` function.

In order to get a rough idea about the magnitude of the effects we can calculate the partial effects at the mean of the data (that is the effect for the average observation). Alternatively, we can calculate the mean of the effects (that is the average of the individual effects). Both can be done with the ```logitmfx(...)``` function from the ```mfx``` package. If we set ```logitmfx(my_logit, data = my_data, atmean = FALSE)``` we calculate the latter, otherwise the former. However, in general we are most interested in the sign and significance of the coefficient. In order to measure the quality of the model overall we can use the Akaike information criterion ("AIC"). It is reported with the summary output for logit models. The absolute value of the AIC has no meaning by itself. However, it can be used to compare and select models. The model with the lowest value is the one that should be chosen according to the AIC. Note that the AIC does not indicate how well the model fits the data. Therefore, we need a measure comparable to the $R^2$ for linear regressions. In fact multiple "Pseudo $R^2$s" have been developed. There are multiple packages that return the $R^2$ given a logit model (see ```rcompanion``` or ```pscl```). The calculation by hand is also fairly simple. This is left as an exercise for the reader.   

## Example

In this example we are going to analyse the probability that somebody cheats on their spouse. As explanatory variables sex, age, religiousness (1 = Anti, 5 = very), self rating of marriage (1 = very unhappy, 5 = very happy) and years married are used. We observe that ones sex does not have a significant impact on the probability of cheating. The older someone gets the less likely it is for them to cheat. More religious people are also less likely to cheat. Unsurprisingly being happier in ones marriage also decreases the likelihood of cheating. On the other hand, the longer someone is married the more likely they are to have cheated. Education does not have a significant impact. If we compare the first model with one that does not include religiousness we notice that the AIC is higher in the latter model. Thus, between the two we would choose the first. Next we calculate the average partial effect using ```logitmfx(...)```. This shows that, on average, giving ones marriage the highest rating decreases the probability of cheating by $\sim 25\%$. Finally, we look at making predictions with our model using ```predict(...)```. The prediction indicates that a 30 year old male who has been married for two years, has 16 years of education, highly rates his marriage and is not very religious has a probability of cheating of $\sim 17\%$ (Try predicting your own likelihood of cheating). Looking at the plot of the prediction shows that the highest density is of probability of cheating is just below $20\%$ and the maximum is above $80\%$. 

```{r}
library(Ecdat)
data("Fair") # needs library(Ecdat)
head(Fair)

# Recoding number of affairs to binary variable
Fair$nbaffairs[Fair$nbaffairs>0] <- 1
# The model
my_logit <- glm(nbaffairs~sex+age+factor(religious)+factor(rate)+ym+education, data = Fair, family = binomial(link = "logit"))
summary(my_logit)

# Take "religious" out. Compare AIC!
my_logit2 <- glm(nbaffairs~sex+age+factor(rate)+ym+education, data = Fair, family = binomial(link = "logit"))
summary(my_logit2)

library(mfx)
# Average partial effect
logitmfx(my_logit, data = Fair, atmean = FALSE)

# Prediction for one observation
predict(my_logit, newdata = data.frame(sex = "male", age = 30, ym = 2, religious = 2, education = 16, rate = 4), type = "response")

# Density of the prediction for the dataset
plot(density(predict(my_logit, type = "response")))
```

```{r echo = FALSE}
logisticPseudoR2s <- function(LogModel) {
  dev <- LogModel$deviance 
  nullDev <- LogModel$null.deviance 
  modelN <- length(LogModel$fitted.values)
  R.l <-  1 -  dev / nullDev
  R.cs <- 1- exp ( -(nullDev - dev) / modelN)
  R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))
  cat("Pseudo R^2 for logistic regression\n")
  cat("Hosmer and Lemeshow R^2  ", round(R.l, 3), "\n")
  cat("Cox and Snell R^2        ", round(R.cs, 3), "\n")
  cat("Nagelkerke R^2           ", round(R.n, 3),    "\n")
}

```

Below three versions of pseudo $R^2$s are reported. They indicate that our model does not describe our data well. So do not worry too much if the model indicates your partner has a high probability of cheating! As an exercise try to write a function that will return the following output:

```{r}
logisticPseudoR2s(my_logit) # try to write this function!!
```

## Perfect Prediction Logit

Perfect prediction occurs whenever a linear function of $X$ can perfectly separate the $1$s from the $0$s in the dependent variable. This is problematic when estimating a logit model as it will result in biased estimators (also check to p-values in the example!). R will return the following message if this occurs:

```glm.fit: fitted probabilities numerically 0 or 1 occurred```

Given this error one should not use the output of the ```glm(...)``` function for the analysis. There are [various ways](https://stats.stackexchange.com/a/68917) to deal with this problem, onw of which is to Firth's bias-Reduced penalized-likelihood logistic regression using the ```logistf(Y~X)``` function in the ```logistf``` package.  

## Example

In this example data $Y = 0$ if $x_1 <0$ and $Y=1$ if $x_1>0$ and we thus have perfect prediction. As we can see the output of the regular logit model is not interpretable. The standard errors are huge compared to the coefficients and thus the p-values are $1$ despite $x_1$ being a predictor of $Y$. Thus we turn to the penalized-likelihood version. This model correctly indicates that $x_1$ is in fact a predictor for $Y$ as the coefficient is significant.  

```{r}
Y <- c(0,0,0,0,1,1,1,1)
X <- cbind(c(-1,-2,-3,-3,5,6,10,11),c(3,2,-1,-1,2,4,1,0))

# Perfect prediction with regular logit
summary(glm(Y~X, family=binomial(link="logit")))

library(logistf)
# Perfect prediction with penalized-likelihood logit
summary(logistf(Y~X))
```

