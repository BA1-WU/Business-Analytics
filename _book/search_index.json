[
["logistic-regression.html", "12 Logistic regression 12.1 Motivation and intuition 12.2 Technical details of the model 12.3 Estimation in R 12.4 Example 12.5 Perfect Prediction Logit 12.6 Example", " 12 Logistic regression 12.1 Motivation and intuition In the last chapter we saw how to predict continuous outcomes (sales, height, etc.) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. To further expand on this, consider the following simulated data. \\(Y\\) is the outcome we want to explain and \\(\\mathbb{X}\\) is our sole predictor. Looking at the scatter plot we can already see that \\(\\mathbb{X}\\) probably has an effect on \\(Y\\). But how can we model it correctly? Figure 12.1: Simulated binary outcome data Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a logistic regression model. As you can see, the linear probability model produces probabilities that are above 1 and below 0, which are not valid probabilites, while the logistic model stays between 0 and 1. Figure 12.2: The same binary data explained by two models; A linear probabilty model (on the left) and a logistic regression model (on the right) A key insight at this point is that the connection between \\(\\mathbb{X}\\) and \\(Y\\) is non-linear in the logistic regression model. As we can see in the plot, the probability of success is most strongly affected by \\(\\mathbb{X}\\) around values of 0, while higher and lower values have a smaller marginal effect. This obviously also has consequences for the interpretation of the coefficients later on. 12.2 Technical details of the model As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form: \\[ f(x) = \\frac{1}{1 + e^{-\\mathbb{X}}} \\] This function transfroms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1. library(latex2exp) x &lt;- seq(-10, 10, length.out = 1000) fx &lt;- 1/(1+exp(-x)) df &lt;- data.frame(x = x, fx = fx) ggplot(df, aes(x = x, y = fx)) + geom_line()+ labs(y = TeX(&quot;$\\\\frac{1}{1+e^{-x}}}$&quot;))+ theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) The \\(\\mathbb{X}\\) in the function above is then replaced by our familiar linear specification, i.e. \\[ P(y_i = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] 12.3 Estimation in R In order to perform a logistic regression in R the glm(Y~X, family = binomial(link = &quot;logit&quot;)) function can be used (you can replace “logit” with “probit” for a probit regression). The coefficients of the model give the change in the log odds of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being \\(1\\) (i.e. uno). We can predict the probability given an observation using the predict(my_logit, newdata = ..., type = &quot;response&quot;) function. In order to get a rough idea about the magnitude of the effects we can calculate the partial effects at the mean of the data (that is the effect for the average observation). Alternatively, we can calculate the mean of the effects (that is the average of the individual effects). Both can be done with the logitmfx(...) function from the mfx package. If we set logitmfx(my_logit, data = my_data, atmean = FALSE) we calculate the latter, otherwise the former. However, in general we are most interested in the sign and significance of the coefficient. In order to measure the quality of the model overall we can use the Akaike information criterion (“AIC”). It is reported with the summary output for logit models. The absolute value of the AIC has no meaning by itself. However, it can be used to compare and select models. The model with the lowest value is the one that should be chosen according to the AIC. Note that the AIC does not indicate how well the model fits the data. Therefore, we need a measure comparable to the \\(R^2\\) for linear regressions. In fact multiple “Pseudo \\(R^2\\)s” have been developed. There are multiple packages that return the \\(R^2\\) given a logit model (see rcompanion or pscl). The calculation by hand is also fairly simple. This is left as an exercise for the reader. 12.4 Example In this example we are going to analyse the probability that somebody cheats on their spouse. As explanatory variables sex, age, religiousness (1 = Anti, 5 = very), self rating of marriage (1 = very unhappy, 5 = very happy) and years married are used. We observe that ones sex does not have a significant impact on the probability of cheating. The older someone gets the less likely it is for them to cheat. More religious people are also less likely to cheat. Unsurprisingly being happier in ones marriage also decreases the likelihood of cheating. On the other hand, the longer someone is married the more likely they are to have cheated. Education does not have a significant impact. If we compare the first model with one that does not include religiousness we notice that the AIC is higher in the latter model. Thus, between the two we would choose the first. Next we calculate the average partial effect using logitmfx(...). This shows that, on average, giving ones marriage the highest rating decreases the probability of cheating by \\(\\sim 25\\%\\). Finally, we look at making predictions with our model using predict(...). The prediction indicates that a 30 year old male who has been married for two years, has 16 years of education, highly rates his marriage and is not very religious has a probability of cheating of \\(\\sim 17\\%\\) (Try predicting your own likelihood of cheating). Looking at the plot of the prediction shows that the highest density is of probability of cheating is just below \\(20\\%\\) and the maximum is above \\(80\\%\\). library(Ecdat) ## Loading required package: Ecfun ## ## Attaching package: &#39;Ecfun&#39; ## The following object is masked from &#39;package:base&#39;: ## ## sign ## ## Attaching package: &#39;Ecdat&#39; ## The following object is masked from &#39;package:datasets&#39;: ## ## Orange data(&quot;Fair&quot;) # needs library(Ecdat) head(Fair) # Recoding number of affairs to binary variable Fair$nbaffairs[Fair$nbaffairs&gt;0] &lt;- 1 # The model my_logit &lt;- glm(nbaffairs~sex+age+factor(religious)+factor(rate)+ym+education, data = Fair, family = binomial(link = &quot;logit&quot;)) summary(my_logit) ## ## Call: ## glm(formula = nbaffairs ~ sex + age + factor(religious) + factor(rate) + ## ym + education, family = binomial(link = &quot;logit&quot;), data = Fair) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.589 -0.743 -0.557 -0.307 2.502 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.8023 0.9663 0.83 0.40637 ## sexmale 0.3416 0.2275 1.50 0.13319 ## age -0.0435 0.0185 -2.36 0.01839 * ## factor(religious)2 -0.9688 0.3662 -2.65 0.00816 ** ## factor(religious)3 -0.5806 0.3694 -1.57 0.11601 ## factor(religious)4 -1.5210 0.3732 -4.08 4.6e-05 *** ## factor(religious)5 -1.4106 0.4530 -3.11 0.00184 ** ## factor(rate)2 0.0623 0.5834 0.11 0.91490 ## factor(rate)3 -0.8266 0.5789 -1.43 0.15333 ## factor(rate)4 -1.1142 0.5590 -1.99 0.04623 * ## factor(rate)5 -1.6456 0.5721 -2.88 0.00402 ** ## ym 0.1095 0.0304 3.60 0.00032 *** ## education 0.0271 0.0463 0.59 0.55783 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 600.47 on 588 degrees of freedom ## AIC: 626.5 ## ## Number of Fisher Scoring iterations: 4 # Take &quot;religious&quot; out. Compare AIC! my_logit2 &lt;- glm(nbaffairs~sex+age+factor(rate)+ym+education, data = Fair, family = binomial(link = &quot;logit&quot;)) summary(my_logit2) ## ## Call: ## glm(formula = nbaffairs ~ sex + age + factor(rate) + ym + education, ## family = binomial(link = &quot;logit&quot;), data = Fair) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.489 -0.765 -0.590 -0.394 2.308 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.0254 0.8986 -0.03 0.97742 ## sexmale 0.3239 0.2229 1.45 0.14620 ## age -0.0462 0.0179 -2.58 0.00996 ** ## factor(rate)2 -0.0050 0.5708 -0.01 0.99301 ## factor(rate)3 -0.9125 0.5664 -1.61 0.10716 ## factor(rate)4 -1.1611 0.5470 -2.12 0.03380 * ## factor(rate)5 -1.7085 0.5576 -3.06 0.00218 ** ## ym 0.0959 0.0290 3.30 0.00096 *** ## education 0.0343 0.0449 0.76 0.44472 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 622.97 on 592 degrees of freedom ## AIC: 641 ## ## Number of Fisher Scoring iterations: 4 library(mfx) ## Loading required package: sandwich ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:Ecdat&#39;: ## ## SP500 ## Loading required package: betareg # Average partial effect logitmfx(my_logit, data = Fair, atmean = FALSE) ## Call: ## logitmfx(formula = my_logit, data = Fair, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## sexmale 0.05598 0.03719 1.51 0.13232 ## age -0.00711 0.00310 -2.29 0.02181 * ## factor(religious)2 -0.14295 0.04765 -3.00 0.00270 ** ## factor(religious)3 -0.08727 0.05055 -1.73 0.08427 . ## factor(religious)4 -0.22033 0.04555 -4.84 1.3e-06 *** ## factor(religious)5 -0.17969 0.04199 -4.28 1.9e-05 *** ## factor(rate)2 0.01031 0.09767 0.11 0.91592 ## factor(rate)3 -0.11895 0.07149 -1.66 0.09611 . ## factor(rate)4 -0.16844 0.07550 -2.23 0.02569 * ## factor(rate)5 -0.24992 0.07726 -3.23 0.00122 ** ## ym 0.01790 0.00529 3.38 0.00071 *** ## education 0.00443 0.00757 0.59 0.55851 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;sexmale&quot; &quot;factor(religious)2&quot; &quot;factor(religious)3&quot; ## [4] &quot;factor(religious)4&quot; &quot;factor(religious)5&quot; &quot;factor(rate)2&quot; ## [7] &quot;factor(rate)3&quot; &quot;factor(rate)4&quot; &quot;factor(rate)5&quot; # Prediction for one observation predict(my_logit, newdata = data.frame(sex = &quot;male&quot;, age = 30, ym = 2, religious = 2, education = 16, rate = 4), type = &quot;response&quot;) ## 1 ## 0.169 # Density of the prediction for the dataset plot(density(predict(my_logit, type = &quot;response&quot;))) Below three versions of pseudo \\(R^2\\)s are reported. They indicate that our model does not describe our data well. So do not worry too much if the model indicates your partner has a high probability of cheating! As an exercise try to write a function that will return the following output: logisticPseudoR2s(my_logit) # try to write this function!! ## Pseudo R^2 for logistic regression ## Hosmer and Lemeshow R^2 0.111 ## Cox and Snell R^2 0.117 ## Nagelkerke R^2 0.174 12.5 Perfect Prediction Logit Perfect prediction occurs whenever a linear function of \\(X\\) can perfectly separate the \\(1\\)s from the \\(0\\)s in the dependent variable. This is problematic when estimating a logit model as it will result in biased estimators (also check to p-values in the example!). R will return the following message if this occurs: glm.fit: fitted probabilities numerically 0 or 1 occurred Given this error one should not use the output of the glm(...) function for the analysis. There are various ways to deal with this problem, onw of which is to Firth’s bias-Reduced penalized-likelihood logistic regression using the logistf(Y~X) function in the logistf package. 12.6 Example In this example data \\(Y = 0\\) if \\(x_1 &lt;0\\) and \\(Y=1\\) if \\(x_1&gt;0\\) and we thus have perfect prediction. As we can see the output of the regular logit model is not interpretable. The standard errors are huge compared to the coefficients and thus the p-values are \\(1\\) despite \\(x_1\\) being a predictor of \\(Y\\). Thus we turn to the penalized-likelihood version. This model correctly indicates that \\(x_1\\) is in fact a predictor for \\(Y\\) as the coefficient is significant. Y &lt;- c(0,0,0,0,1,1,1,1) X &lt;- cbind(c(-1,-2,-3,-3,5,6,10,11),c(3,2,-1,-1,2,4,1,0)) # Perfect prediction with regular logit summary(glm(Y~X, family=binomial(link=&quot;logit&quot;))) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## ## Call: ## glm(formula = Y ~ X, family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## 1 2 3 4 5 6 ## -1.02e-05 -1.23e-06 -3.37e-06 -3.37e-06 1.06e-05 6.08e-06 ## 7 8 ## 2.10e-08 2.10e-08 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.94 113859.81 0 1 ## X1 7.36 15925.25 0 1 ## X2 -3.12 43853.49 0 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1.1090e+01 on 7 degrees of freedom ## Residual deviance: 2.7772e-10 on 5 degrees of freedom ## AIC: 6 ## ## Number of Fisher Scoring iterations: 24 library(logistf) # Perfect prediction with penalized-likelihood logit summary(logistf(Y~X)) ## logistf(formula = Y ~ X) ## ## Model fitted by Penalized ML ## Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood ## ## coef se(coef) lower 0.95 upper 0.95 Chisq p ## (Intercept) -0.9887 1.428 -10.2169 1.88 0.5923 0.4415 ## X1 0.3320 0.214 0.0417 1.46 5.3158 0.0211 ## X2 0.0825 0.609 -2.1789 3.38 0.0198 0.8881 ## ## Likelihood ratio test=5.8 on 2 df, p=0.055, n=8 ## Wald test = 2.71 on 2 df, p = 0.258 ## ## Covariance-Matrix: ## [,1] [,2] [,3] ## [1,] 2.0402 -0.0456 -0.5676 ## [2,] -0.0456 0.0459 -0.0336 ## [3,] -0.5676 -0.0336 0.3703 "]
]
