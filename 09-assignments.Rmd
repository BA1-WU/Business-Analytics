---
title: "09-Assignments"
output:
  html_document:
    toc: yes
  html_notebook: default
  pdf_document:
    toc: yes
---

# Assignments

**Note that depending on which data set you received, your results might differ slightly from the results presented here!**

## Assignment 2 (Hypothesis Testing)

As a marketing manager at a video streaming service, you are interested in the effect of online advertising on the number of streams that a movie receives. To test the effect of online advertising on streams, you select a representative sample of 200 movies and randomly assign 100 movies to be included in an online advertising campaign. The other half of the sample serves as the control group. You run the experiment for one week and collect data regarding the number of streams from this period. In addition, you collect other variables about the movies that you wish to analyze. Overall, the data set includes the following variables:

* movieID: unique movie ID
* streams_sd: number of streams in SD-quality
* streams_hd: number of streams in HD-quality
* online_advertising: indicator whether a movie was included in the online advertising campaign (0 = no, 1 = yes) 
* customer_rating: satisfaction measured on a 5-point Likert scale after the moviews were watched: (1) "very dissatisfied" â€“ (5) "very satisfied""
* academy_award: 0= non-nominated film, 1=nominated film
* genre: 1 = drama, 2 = romance, 3 = comedy, 4 = thriller, 5 = action

Apply appropriate statistical methods to answer the following questions:

1. Is there a significant difference in streams between movies that were included in the online advertising campaign and those that were not included? (Please conduct the test for SD and HD movies)
2. Is there a significant difference in streams between movies in HD and SD quality?
3. Is there a significant association between the genre of a movie and the academy award nomination?
4. Is there a significant difference in customer ratings between nominated and non-nominated movies?

When answering the questions, please remember to address the following points:

* Formulate the corresponding hypotheses and choose an appropriate statistical test
* Provide the reason for your choice and discuss if the assumptions of the test are met 
* Convert the variables to the appropriate type (e.g., factor variables)
* Create appropriate graphs to explore the data (e.g., boxplot, bar chart, histogram)
* Provide appropriate descriptive statistics for the variables
* Report and interpret the test results accurately (including confidence intervals where appropriate)  
* Finally, don't forget to report your research conclusion in an appropriate way

When you are done with your analysis, click on "Knit to HTML" button above the code editor. This will create a HTML document of your results in the folder where the "assignment.Rmd" file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn\@WU. The file name should be "assignment2_studendID_name.html".

### Load data

```{r load_data}
movie_data <- read.table("https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/assignment1.2.dat", 
                          sep = "\t", 
                          header = TRUE) #read in data
head(movie_data)
str(movie_data)
```

The first step is to load the required packages for visualization and descriptive statistcs.   

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
library(psych)
```

Note that these packages should be installed prior to creating the HTML output from the .Rmd file.

### Question 1

First we will analyse whether the advertising campaign had an effect on SD streams. We need to formulate a hypothesis which we can test. In this case the null hypothesis is that the campaign had no effect on the mean number of streams, i.e. that there is no difference in the mean number of streams. The alternative hypothesis states that the campaign _did_ have an effect, meaning that there is a difference in the mean number of streams. In more formal notation this is:

$$H_0: \mu_0 = \mu_1 \\ H_1: \mu_0 \neq \mu_1$$

We need to transform the variable online_advertising into a factor variable for some of our applications.

```{r warning = F, message = F}
# Transform into factor variable
movie_data$online_advertising <- factor(movie_data$online_advertising, levels = c(0,1), labels = c("no", "yes"))
```

A good way to get a feeling for the data is to create descriptive statistics and appropriate plots (e.g., plot of means, boxplot).

```{r warning = F, message = F}
# Descriptive statistics for SD streams, split by online advertising
stats <- describeBy(movie_data$streams_sd, movie_data$online_advertising)
print(stats)

# Boxplots for SD streams, split by online advertising
ggplot(movie_data, aes(y = streams_sd, x = online_advertising)) + 
  geom_boxplot() + 
  labs(x = "Online Advertising", y = "SD streams") + 
  theme_bw()

ggplot(movie_data, aes(online_advertising, streams_sd)) + 
  geom_bar(stat = "summary",  color = "black", fill = "white", width = 0.7) +
  geom_pointrange(stat = "summary") + 
  labs(x = "Online Advertising", y = "SD streams") +
  theme_bw()
```

As we can see in both the descriptive statistics and the plots, the mean of the number of streams is higher where online_advertising = 1, i.e. for the videos that were included in the marketing campaign. To test whether or not this difference is significant, we need to use a __two sample t-test__. The requirements are clearly met:

* Our dependent value is on a ratio scale.
* Since we have more than 30 observations per group we do not really have to concern ourselves with whether the data is normally distributed or not.
* If a video was inluded in the campaign or not was assigned randomly.
* Since we have equal group sizes, the homogeneity of variance assumption is not too relevant. Furthermore, R automatically performs Welch's t-test, which corrects for unequal variance anyway. 

Thus we can perform the test in R

```{r warning = F, message = F}
t.test(streams_sd ~ online_advertising, data = movie_data, paired = FALSE)
```

The test is significant for any sensible level of confidence, leading us to reject the null hypothesis that there is no difference in the mean number of streams. In effect, this means that the advertising campaign had an effect on the average number of times a video was streamed. Another thing we can extract from this test result is the confidence interval around the difference in means. We are 95% certain that the true difference of means lies somewhere between -1003.213 and -1500.967.

The same can be done analogously for HD streams:
```{r warning = F, message = F}
# Descriptive statistics for HD streams, split by online advertising
stats <- describeBy(movie_data$streams_hd, movie_data$online_advertising)
print(stats)

# Boxplots for HD streams, split by online advertising
ggplot(movie_data, aes(y = streams_hd, x = online_advertising)) + 
  geom_boxplot() + 
  labs(x = "Online Advertising", y = "HD streams") + 
  theme_bw()

ggplot(movie_data, aes(online_advertising, streams_hd)) + 
  geom_bar(stat = "summary",  color = "black", fill = "white", width = 0.7) +
  geom_pointrange(stat = "summary") + 
  labs(x = "Online Advertising", y = "HD streams") +
  theme_bw()
```

Again, the summary statistics and boxplots seem to indicate that there is a difference in means. Using the same reasoning as before, we can deduce that we need a two sample t-test to determine whether this difference is signficant.

```{r warning = F, message = F}
t.test(streams_hd ~ online_advertising, data = movie_data, paired = FALSE)

```

Again, the p-value is so low that any sensible signifcance level would lead us to reject the null hypothesis, leading us to the conclusion that there is a difference in mean number of streams between videos included in the campaign and those that aren't. Looking again at the 95 percent confidence intervals, however, it would appear that the effect was stronger for SD streams than for HD streams, since here we are 95 percent certain that the true difference in means lies between -821.2123 and -519.3277.

### Question 2

Next we want to examine whether HD and SD streams have similar numbers o on average. The null hypothesis here is that there is no difference in the mean number of HD streams and the mean number of SD streams, with the alternative hypothesis is that such a difference exists. In mathematical notation this implies

$$H_0: \mu_0 = \mu_1 \\ H_1: \mu_0 \neq \mu_1$$

Again, we start with descriptive statistics to get a feel for the data.

```{r warning = F, message = F, paged.print = FALSE}
# Descriptive statistics for HD and SD streams
stats_sd <- psych::describe(movie_data$streams_sd)
print(stats_sd)

stats_hd <- psych::describe(movie_data$streams_hd)
print(stats_hd)


# Boxplots
ggplot(data = movie_data) +
  geom_boxplot(aes(y = streams_sd, x = "SD")) + 
  geom_boxplot(aes(y = streams_hd, x = "HD")) + 
  theme_bw() + 
  labs(x = "", y = "Streams")

ggplot(movie_data) + 
  geom_bar(aes(y = streams_sd, x = "SD"),stat = "summary",  color = "black", fill = "white", width = 0.7) +
  geom_pointrange(aes(y = streams_sd, x = "SD"),stat = "summary") + 
  geom_bar(aes(y = streams_hd, x = "HD"),stat = "summary",  color = "black", fill = "white", width = 0.7) +
  geom_pointrange(aes(y = streams_hd, x = "HD"), stat = "summary") + 
  labs(x = "Video qualtity", y = "Number of streams") +
  theme_bw()

```

As in question one, it appears that there is a difference in the means. To test whether it is significant we again need a t-test, with one important distinction: Our observations were in both experimental conditions, meaning we have data on HD and SD streams for each observation. This means that we need a __dependent means t-test__. The other assumptions are virtually identical to the regular t-test. The test can be executed in R by adding ```paired = TRUE``` to the code.   

```{r warning = F, message = F}
t.test(y = movie_data$streams_sd, x = movie_data$streams_hd, paired = TRUE)
```

The p-value is again lower than any sensible signifance level, which means that we reject the null hypothesis that there is no difference in the mean number of streams. Interesting to note again is the 95 percent confidence interval, which states that we are 95 percent certain that the true difference of means lies between -990.5032 and -712.5568.

### Question 3

Next we want to know whether there is a significant association between the genre of a movie and the academy award nomination. This is essentially a question of **dependence**, which means that our null hypothesis is that the two variables are independent and the alternative hypothesis is that they are dependent.

First we need to reshape the first variable into a factor and, while we're at it, give the factor levels more descriptive names.

```{r warning = F, message = F}
# Rename factor variable for subsequent plot
movie_data$genre <- factor(movie_data$genre, c(1,2,3,4,5), c("Drama", "Romance", "Comedy", "Thriller", "Action"))
```

The next step is to produce a relative frequency table and an appropriate visualization - in this case a stacked bar plot.

```{r warning = F, message = F}
# Create relative frequency table
rel_freq_table <- as.data.frame(prop.table(table(movie_data[,c("genre", "academy_award")]), 1))

rel_freq_table$academy_award <- factor(rel_freq_table$academy_award, levels = c(0,1), labels = c("No", "Yes"))


ggplot(rel_freq_table, aes(x = genre, y = Freq, fill = academy_award)) + #plot data
  geom_col(width = .7) + #position
  geom_text(aes(label = paste0(round(Freq*100,0),"%")), position = position_stack(vjust = 0.5), size = 4) + #add percentages
  ylab("Academy award (proportion)") + xlab("Genre") + # specify axis labels
  theme_bw() + 
  scale_fill_discrete(name = "Academy award")
```

If the two variables truly are independent, knowing the genre would tell us nothing about the probability of an academy award. The relative frequency table and the above plot hint at the fact that this may not be the case. To actually test this we use the __Chi-squared test__ . 

```{r warning = F, message = F}
# Create contingency table for Chi-squared test
cont_table <- table(movie_data[,c("genre", "academy_award")])
print(cont_table)

chisq.test(cont_table)
```

Applying the widely used confidence level of 0.05 leads us to reject the null hypothesis in this case, with the alternative hypothesis being that the variables ```genre``` and ```academy_award``` are actually dependent. 

### Question 4

Finally we want to examine whether there is a difference in customer ratings between movies that wer nominated for an academy award and those that were not. In this case our null hypothesis is that both have a similar distribution in regard to their customer rating. The alternative hypothesis is that they are not drawn from the same distribution. 

First we create a table with the conditional relative frequencies, which enables us to plot the data.

```{r warning = F, message = F}
# Create table with conditional relative frequencies
table_plot_cond_rel <- as.data.frame(prop.table(table(movie_data[,c("academy_award","customer_rating")]), 1)) 

#Rename factor levels to make subsequent plot nicer
table_plot_cond_rel$academy_award <- factor(table_plot_cond_rel$academy_award, levels = c(0,1), c("No academy award", "Academy award"))
```

The next step is to create a barplot of the conditional relative frequencies, to give us an overview of the distribution of the data. 

```{r warning = F, message = F}
# Barplot split by academy award
ggplot(table_plot_cond_rel, aes(x = customer_rating, y = Freq)) + 
  geom_col() + 
  facet_wrap(~ academy_award) + 
  theme_bw() + 
  labs(x = "Customer rating", y = "Relative frequency")
```

Based solely on this plot it seems that movies with academy awards are more likely to gain higher customer ratings. To be able to test this, we need the Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test). Obviously, we need to check whether the assumptions for the test are fulfilled. 

* Our dependent variable is ordinally scaled (i.e. there is a "natural" order to customer satisfaction ratings).
* The independent variable is a factor with two levels.
* The data comes from two independent samples.

The reason why the dependent t-test is not appropriate is because the dependent variable is not measured on a continuous scale (i.e., inveral or ratio). 

To perform the Mann-Whitney U Test we use the function ```wilcox.test()```.

```{r warning = F, message = F}
# Perform Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)
wilcox.test(customer_rating ~ academy_award, data = movie_data, paired = FALSE)
```

Using a confidence level of 0.05, we cannot reject the null hypothesis in this case, meaning we would retain the assumption that customer ratings for movies with and without academy award nomination are drawn from the same distribution.

Note that a using a different confidence level (e.g., 0.1)  we could speak of a marginally  significant result. 

## Assignment 3 (Anova)

The data file contains customer information from an online fashion shop. In an experiment, the customers were exposed to different types of online advertising over the past year (randomly assigned) and now you wish to analyze the results.

The following variables are included in the data set:

* customerID: unique customer ID
* revenue: revenue per cusomer for the past year (in EUR)
* gender: 0=male, 1=female
* retargeting: type of online advertising that the customer was exposed to (3 levels: 1 = no advertising, 2 = generic retargeting, 3 = dynamic retargeting)
* customerRank: ranking of customers according to their expenditure level (low rank = valuable customer, high rank = less valuable customer) 

Use R and appropriate analytical techniques to answer the following questions:

1. Has the types of online advertising an effect on revenue? Are there significant differences between the individual groups?
2. Does the effect of online advertising on revenue depend on gender? (conduct a 2 x 3 analysis of variance with "revenue" as the dependent variable and "gender" and "retargeting" as the independent variables or factors.)
3. Is the customer ranking significantly influenced by the type of online advertising? Are there significant differences between the individual groups?

When answering the questions, please remember to address the following points:

* Formulate the corresponding hypotheses and choose an appropriate statistical test
* Provide the reason for your choice and discuss if the assumptions of the test are met 
* Convert the variables to the appropriate type (e.g., factor variables)
* Create appropriate graphs to explore the data (e.g., boxplot, bar chart)
* Provide appropriate descriptive statistics for the variables
* Report and interpret the test results accurately
* Finally, don't forget to report your research conclusion in an appropriate way

When you are done with your analysis, click on "Knit to HTML" button above the code editor. This will create a HTML document of your results in the folder where the "assignment3.Rmd" file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn\@WU. The file name should be "assignment3_studendID_name.html".

### Load data

As always, the first step is to load required packages (packages that have not been used as often in the course will be loaded as required to show which packages contain certain functions) and to load and inspect the data. 

```{r warning=FALSE, message=FALSE}
library(plyr)
library(ggplot2)
library(pastecs)

dld.full <- read.table("https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/assignment3.6.dat", 
                            sep = "\t", 
                            header = TRUE) #read in data
head(dld.full)
str(dld.full)
```

Next we are going to recode some of the variables into factors and give them more descriptive level names. 

```{r, warning=FALSE, message=FALSE}
dld.full$retargeting <- factor(dld.full$retargeting, levels = c(1,2,3), labels = c("no retargeting", "generic retargeting", "dynamic retargeting"))
dld.full$gender <- factor(dld.full$gender, levels = c(1,0),labels = c("female","male"))
```

### Question 1

To answer whether the type of advertising has an effect on revenue we need to formulate a testable null hypothesis. In our case the null hypothesis is stating that the average level of sales is equal for all advertising types. In mathematical notation this implies:

$$H_0: \mu_1 = \mu_2 = \mu_3 $$

The alternate hypothesis is simply that the means are not all equal, i.e., 

$$H_1: \textrm{Means are not all equal}$$  

If you wanted to put this in mathematical notation, you could also write:

$$H_1: \exists {i,j}: {\mu_i \ne \mu_j} $$  

However, this was not required. 

The appropriate test for such a hypothesis is one-way ANOVA since we have a metric scales dependent variable and a categorical independent variable with more than two levels.

Next we will calculate summary statistics for the data and produce an approppriate plot.

```{r fig.align="center"}
by(dld.full$revenue,dld.full$retargeting,stat.desc)
ggplot(dld.full, aes(retargeting, revenue)) + 
  stat_summary(fun.y = mean, geom = "bar", fill = "White", colour = "Black") +
  stat_summary(fun.data = mean_cl_normal, geom = "pointrange") + 
  labs(x = "Experimental group (promotion level)", y = "Number of sales") + 
  theme_bw()

```

Both the summary statistics and the plot hint at the fact that the means may not be equal. Before we move to the test, we need to see if a series of assumptions are met, namely:

* Distributional assumptions
* Homogeneity of variances
* Indepnedence of observations

The last assumption is satisfied due to the fact that the observations were randomly assigned to the advertisement groups. To see if we need to worry about distributional assumptions we first take a look at the number of observations in each advertising group.

```{r, warning=FALSE, message=FALSE}
#check number of observations by group
table(dld.full$retargeting)
```

Due to the fact that there always more than 30 observations in each group we can rely on the central limit theorem to satisfy the distributional assumptions. 

Homogeneity of variances can be checked with Levene's test (implemented as ```leveneTest()``` from the ```car``` package). The null hypothesis of this test is that the variances are equal, with the alternative hypothesis being that the variances not all equal. 

```{r, warning=FALSE, message=FALSE, paged.print = FALSE}
#Homogeneity of variances test:
library(car)
leveneTest(revenue ~ retargeting, data=dld.full, center=mean)
```

The test is not significant (for a signifcance level of 5 %), meaning that we do not reject the null hypothesis of equal variances and can operate under the assumption that the variances are equal. 

Since all assumptions are fulfilled we can move on to conducting the actual ANOVA using the ```aov()``` function.

```{r, warning=FALSE, message=FALSE}
#Anova:
aov <- aov(revenue~retargeting, data = dld.full)
summary(aov)
```

The p-value is below any sensible level of signficance, which means that we reject the null hypothesis of the means being equal in the three advertising groups. 

Next we will briefly inspect the residuals of the ANOVA to see if the assumptions of the test really are justified.

```{r, warning=FALSE, message=FALSE}
#Inspect residuals
plot(aov,1)
```

The first plot gives us a feel for the distribution of the residuals of the three groups. The residuals seem to be roughly equally distributed, which speaks for the fact that the homogeneity of variances assumptions is fulfilled. 


```{r, warning=FALSE, message=FALSE}
plot(aov,2)
```

The second plot is a QQ-plot of the residuals, meant as a quick visual check to see if the normality assumption is fulfilled. Leading up to the test we only checked if there were more than 30 observations per group to satisfy the normality assumption but despite this being fulfilled it is still important to check the normality of the residuals, as any strange behaviour here may indicate problems with the model specification. 

To further confirm that the residuals are roughly normally distributed we employ the Shapiro-Wilk test. The null hypothesis is that the distribution of the data is normal, with the alternative hypothesis positing that the data is not normally distributed.

```{r, warning=FALSE, message=FALSE}
shapiro.test(resid(aov))
```

The p value is far above any widely used significance level and thus we can not reject the null hypothesis of normal distribution, which further implies that the normality assumption is fulfilled.

The ANOVA result only tells us that the means of the three groups are not equal, but it does not tell us anything about _which_ pairs of means are unequal. To find this out we need to conduct post hoc tests. Here we will conduct both the Bonferroni correction as well as Tukey's HSD test, however either would be sufficient for your homework. 

Bonferroni's correction conducts multiple pairwise t-tests, with the null hypothesis being that of equal means in each case and the alternative hypothesis stating that the means are unequal.

```{r, warning=FALSE, message=FALSE}
#bonferroni
pairwise.t.test(dld.full$revenue, dld.full$retargeting, data=dld.full, p.adjust.method = "bonferroni")
```

All of the p-values in our output are well below 0.05, meaning that we reject the null hypothesis in all three cases. 

Tukey's HSD similarly compares pairwise means, corrected for family-wise errors. 

```{r, warning=FALSE, message=FALSE}
#tukey correction using the mult-comp package
library(multcomp)
tukeys <- glht(aov,linfct=mcp(retargeting = "Tukey"))
summary(tukeys)
```

As in the case of Bonferroni's correction, all three p-values are well below 0.05 and thus we reject the null hypothesis of equal means. Tukey's HSD further lets us estimate the difference in means with corresponding confidence intervals.

```{r, warning=FALSE, message=FALSE}
confint(tukeys)
# The mar parameter changes the margins around created plots. This is done so the labels on the side of the Tukey plot are visible (however, this was not expected). 
par(mar = c(5, 20, 4, 2))
plot(tukeys)
par(mar = c(5, 4, 4, 2))
```

It is clearly visible that none of the CIs cross the 0 bound, which further indicates that the differences in means are statistically significantly different from 0.

From a reporting standpoint we can say that revenue is higher when using generic retargeting vs. no retargeting, dynamic retargeting vs. no retargeting and also when using dynamic retargeting vs. generic retargeting.

### Question 2

Next we want to analyse whether gender interacts with retargeting when it comes to revenue. The correct tool for this analysis is n-way ANOVA (a.k.a factorial ANOVA). As usual, we start off with some descriptive statistics and plots to get a feel for the data. 

```{r fig.align="center"}
#create new grouping variable
dld.full$Group <- paste(dld.full$retargeting, dld.full$gender, sep = "_") 
#check descriptives and plot data
by(dld.full$revenue,dld.full$Group,stat.desc)

ggplot(dld.full, aes(gender, revenue)) + 
  stat_summary(fun.y = mean, geom = "bar", fill = "White", colour = "Black") +
  stat_summary(fun.data = mean_cl_normal, geom = "pointrange") + 
  labs(x = "Gender", y = "Average revenue") + 
  theme_bw()

ggplot(dld.full, aes(x = interaction(gender, retargeting), y = revenue, fill = gender)) +
  stat_summary(fun.y = mean, geom = "bar", position = position_dodge()) +
  stat_summary(fun.data = mean_cl_normal, geom = "pointrange") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

```

Both the summary statistics and the plots seem to indicate a difference between men and women. Before we can jump straight into the analysis, we need to test whether the assumptions of the model are justified. 

We begin with the distributional assumptions. To this end we check how many observations are in each group.
```{r, warning=FALSE, message=FALSE}
#check number of obs per group
table(dld.full$Group)
```

Since there are more than 30 observations in each group, we can again rely on the central limit theorem to satisfy the normality assumption. Next is the homogeneity of variances assumption, which we again test using Levene's test.

```{r, warning=FALSE, message=FALSE, paged.print = FALSE}
#test homogeneity of variances
leveneTest(dld.full$revenue, interaction(dld.full$retargeting,dld.full$gender), center=mean)
```

Again the p-value is far above any widely used significance level and we can not reject the null hypothesis of equal variances. As the necessary assumptions are fulfilled, we can move on to the factorial ANOVA.

```{r, warning=FALSE, message=FALSE}
aov <- aov(revenue~retargeting+gender+retargeting:gender, data = dld.full)
summary(aov)
```

Since the interaction effect is significant, we can no longer interpret the main effects and can only resort to saying that the effect of one factor depends on the other, i.e. that gender has an effect on the level of revenue in each advertising group. This means that gender has an effect on the revenue generated in each advertising group. 

As a final step we again check the residuals to see whether or not the assumptions are justified.

```{r, warning=FALSE, message=FALSE}
#Inspect residuals
plot(aov,1)
plot(aov,2)
shapiro.test(resid(aov))
```

Both assumptions (homogeneity of variances and the normality assumption) seem to be justifiable through the residuals. 


### Question 3

For the last question we want to examine whether customer ranks are signifcantly different for different types of advertising. Because we are dealing with data on an ordinal scale, we can not use ANOVA for this type of question. The non-parametric counterpart is the Kruskal-Wallis test, which tests for differences in medians between groups. Hence, the null hypothesis is that the medians are equal in each group and the alternative hypothesis is that the medians are unequal. 

$$H_0: \tilde{x}_1 =  \tilde{x}_2 = \tilde{x}_3 $$
$$H_1: \textrm{The meadians are not all equal} $$
A good way to visualize ordinal data is through a boxplot.

```{r, fig.align="center"}
ggplot(data = dld.full, aes(x = retargeting, y = rank)) + 
  geom_boxplot() + 
  theme_bw() + 
  labs(x = "", y = "Rank")
```

The boxplot seems to indicate that the medians are unequal. 

The only assumption that we require for this test is that the dependent variable is at least ordinal, which is fulfilled for customer ranks. Hence we can move on to performing the test in R.

```{r, warning=FALSE, message=FALSE}
#ordinal data so we use a non-parametric test
kruskal.test(rank ~ retargeting, data = dld.full)
```

The p-value is again below any sensible signifcance level and thus we reject the null hypothesis of equal medians. This means that the median rank of customers is different for different types of retargeting, implying that the type of retargeting has an effect on the customer rank.

To further see which of the medians are unequeal we perform the Nemenyi post hoc test, which can be found in the ```PCMCR``` package in R. The null hyptohesis is that the pairwise medians are equal, while the alternative hypothesis is that the pairwise medians are unequal. 

```{r, warning=FALSE, message=FALSE}
library(PMCMR)
posthoc.kruskal.nemenyi.test(x = dld.full$rank, g = dld.full$retargeting, dist = "Tukey")
```

All of the p-values are well below 0.05, meaning that the pairwise differences between the medians are always signifianct. This in turn implies that every type of advertising leads to different customer ranks. 

  
## Assignment 4 (Regression)
  
As a marketing manager of a consumer electronics company, you are assigned the task to analyze the relative influence of different marketing activities. Specifically, you are supposed to analyse the effects of (1) TV advertising, (2) online advertising, and (3) radio advertising on the sales of fitness trackers (wristbands). Your data set consists of sales of the product in different markets (each line represents one market) from the past year, along with the advertising budgets for the product in each of those markets for three different media: TV, online, and radio. 

The following variables are available to you:
  
* Sales (in thousands of units)
* TV advertising budget (in thousands of Euros)
* Online advertising budget (in thousands of Euros)
* Radio advertising budget (in thousands of Euros)

Please conduct the following analyses: 
  
1. Formally state the regression equation, which you will use to determine the relative influence of the marketing activities on sales.
2. Describe the model variables using appropriate statstics and plots
3. Estimate a multiple linear regression model to determine the relative influence of each of the variables. Before you interpret the results, test if the model assumptions are fulfilled and use appropriate tests and plots to test the assumptions.
4. Interpret the model results:
* Which variables have a significant influence on sales and what is the interpretation of the coefficients?
* What is the relative importance of the predictor variables?
* Interpret the F-test
* How do you judge the fit of the model? Please also visualize the model fit using an appropriate graph.
5. What sales quantity would you predict based on your model for a product when the marketing activities are planned as follows: TV: 150 thsd. â‚¬, Online: 26 thsd. â‚¬, Radio: 15 thsd. â‚¬? Please provide the equation you used to make the prediction. 

When you are done with your analysis, click on "Knit to HTML" button above the code editor. This will create a HTML document of your results in the folder where the "assignment4.Rmd" file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn\@WU. The file name should be "assignment4_studendID_name.html".

### Load data

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
sales_data <- read.table("https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/assignment4.2.dat", 
                         sep = "\t", 
                         header = TRUE) #read in data
sales_data$market_id <- 1:nrow(sales_data)
head(sales_data)
str(sales_data)
```

### Question 1

In a first step, we specifiy the regression equation. In this case, sales is the dependend variable which is regressed on the different types of advertising expenditures that represent the independent variables. Thus, the regression equation is:

$$Sales=\beta_0 + \beta_1 * tvadspend + \beta_2 * onlineadspend + \beta_3 * radioadspend + \epsilon$$
  
### Question 2
  
The descriptive statistics can be checked using the ```describe()``` function:
  
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE, paged.print = FALSE}
library(psych)
psych::describe(sales_data)
```

Since we have continuous variables, we use scatterplots to investigate the relationship between sales and each of the predictor variables.

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
library(ggplot2)
ggplot(sales_data, aes(x = tv_adspend, y = sales)) + geom_point() +theme_bw()
ggplot(sales_data, aes(x = online_adspend, y = sales)) + geom_point() +theme_bw()
ggplot(sales_data, aes(x = radio_adspend, y = sales)) + geom_point() +theme_bw()
```

The plots already suggest that there might be a positive linear relationship between sales and TV- and online-advertising. However, there does not appear to be a strong relationship between sales and radio advertising. 

### Question 3
  
The estimate the model, we will use the ```lm()``` function:
  
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
linear_model <- lm(sales ~ tv_adspend + online_adspend + radio_adspend, data = sales_data)
```

Before we can inspect the results, we need to test if there might be potential problems with our model specification. 

#### Outliers

The check for outliers, we extract the studentized residuals from our model and test if there are any absolute values larger than 3. 
  
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
sales_data$stud_resid <- rstudent(linear_model)
plot(1:nrow(sales_data),sales_data$stud_resid, ylim=c(-3.3,3.3)) #create scatterplot 
abline(h=c(-3,3),col="red",lty=2) #add reference lines
```

Since there are no residuals with absolut values larger than 3, we conclude that there are no severe outliers. 

#### Influencial observations

To test for influential observations, we use Cook's Distance. You may use the following two plots to verify if any Cook's Dinstance values are larger than the cutoff of 1. 

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(linear_model,4)
plot(linear_model,5)
```

Since all values are well below the cutoff, we conclude that influencial observations are not a problem in our model. 

#### Non-linear relationships

Next, we test if a linear specification appears feasible. You could test this using the added variable plots:

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
library(car)
avPlots(linear_model)
```

The plots suggest that the linear specification is appropriate. In addition, you could also use the residuals plot to see if the linear specification is appropriate. The red line is a smoothed curve through the residuals plot and if it deviates from the dashed grey horizontal line a lot, this would suggest that a linear specification is not appropriate. 

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(linear_model, 1)
```

In this example, the red line is close to the dashed grey line, so the linear specification appears reasonable. 

#### Heteroscedasticity

Next, we test if the residual variance is approximately the same at all levels of the predicted outcome variables (i.e., homoscedasticity). To do this, we use the residuals plot again.

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(linear_model, 1)
```

The spread of residuals at different levels of the predicted outcome does not appear to be very different. Thus, we can conclude that heteroscedasticity is unlikely to be a problem. 

#### Non-normally distributed errors

Next, we test if the residuals are approximately normally distributed using the Q-Q plot from the output:

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(linear_model,2)
```

The Q-Q plot does not suggest a severe deviation from a normal distribution. This could also be validated using the Shapiro test:

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
shapiro.test(resid(linear_model))
```

#### Correlation of errors

Next, we test if the residuals are uncorrelated. Any clustered pattern in the residuals plot would indicate dependence of errors. 

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(linear_model,1)
```

In our example, the residuals do not appear to be correlated since there is no clustered pattern.

#### Multicollinearity

To test for linear dependence of the regressors, we first test the bivariate correlations for any extremely high correlations (i.e., >0.8).

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
library("Hmisc")
rcorr(as.matrix(sales_data[,c("tv_adspend","online_adspend","radio_adspend")]))
```

The results show that the bivariate correlations are rather low. This can also be shown in a plot:

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
plot(sales_data[,c("tv_adspend","online_adspend","radio_adspend")])
```

In a next step, we compute the variance inflation factor for each predictor variable. The values should be close to 1 and values larger than 4 indicate potential problems with the linear dependence of regressors.  

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
library(car)
vif(linear_model)
```

Here, all vif values are well below the cutoff, indicating that there are no problems with multicollinearity. 

### Question 4

In a next step, we will investigate the results from the model using the ```summary()``` function. 
  
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
summary(linear_model)
```

For each of the individual predictors, we test the following hypothesis: 

$$H_0: \beta_k=0$$
$$H_1: \beta_k\ne0$$

where k denotes the number of the regression coefficient. In the present example, we reject the null hypothesis for the first two predictors, where we observe a significant effect (i.e., p < 0.05 for "tv_adspend" and "online_adspend"). However, we fail to reject the null for the "radio_adspend" variable (i.e., the effect is insignificant). 

The interpretation of the coefficients is as follows: 

* tv_adspend (&beta;<sub>1</sub>): when tv advertising expenditures increase by 1 Euro, sales will increase by `r round(summary(linear_model)$coefficients[2],3)` units
* online_adspend (&beta;<sub>2</sub>): when online advertising expenditures increase by 1 Euro, sales will increase by `r round(summary(linear_model)$coefficients[3],3)` units
* radio_adspend (&beta;<sub>3</sub>): when radio advertising expenditures increase by 1 Euro, sales will increase by `r round(summary(linear_model)$coefficients[4],3)` units

You should always provide a measure of uncertainty that is associated with the estimates. You could compute the confidence intervals around the coefficients using the ```confint()``` function.

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
confint(linear_model)
```

The results show that, for example, with a 95% probability the effect of online advertising will be between 0.172 and 0.211. 

Although the variables are measured on the same scale, you should still test the relative influence by inspecting the standardized coefficients that express the effects in terms of standard deviations.  

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
library(lm.beta)
lm.beta(linear_model)
```

Here, we conclude that tv advertising has the largest ROI followed by online advertising and radio advertising. 

Another significance test is the F-test. It tests the null hypothesis:

$$H_0: R^2=0$$

This is equivalent to the following null hypothesis: 

$$H_0: \beta_1=\beta_2=\beta_3=\beta_k=0$$

The result of the test is provided in the output above ("F-statistic: 363.7 on 3 and 236 DF,  p-value: < 2.2e-16"). Since the p-value is smaller than 0.05, we reject the null hypothesis that all coefficients are zero. 

If you would like to make it more explicit, you could use the ```anova()``` function to get the specific anova results.

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE, paged.print = FALSE}
anova(linear_model)
```

Regarding the model fit, the R<sup>2</sup> statistic tells us that approximately 82% of the variance can be explained by the model. This can be visualized as follows: 

```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
sales_data$yhat <- predict(linear_model)
ggplot(sales_data,aes(yhat,sales)) +  
  geom_point(size=2,shape=1) +  #Use hollow circles
  scale_x_continuous(name="predicted values") +
  scale_y_continuous(name="observed values") +
  geom_abline(intercept = 0, slope = 1) +
  theme_bw()
```

### Question 5
  
Finally, we can predict the outcome for the given marketing mix using the following equation: 
  
$$\hat{sales}= 0.045*150 + 0.192*26 + 0.007*15 = 14.623$$
  
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE}
summary(linear_model)$coefficients[1,1] + 
  summary(linear_model)$coefficients[2,1]*150 + 
  summary(linear_model)$coefficients[3,1]*26 + 
  summary(linear_model)$coefficients[4,1]*15
```

This means that given the planned marketing mix, we would expect to seel around 14,623 units. 

