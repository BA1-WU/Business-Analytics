---
output:
  html_notebook: default
---

# Probability Distributions
This chapter is primarily based on:

* Casella, G., & Berger, R. L. (2002). Statistical inference (Vol. 2). Pacific Grove, CA: Duxbury (**chapters 2&3**).

## Introduction

On the previous page we talked about probability density/mass functions (PDFs/PMFs) and cumulative distribution functions (CDFs). We have also shown some plots of those functions. Here, we want to explore probability distributions that can help us describe the PDFs and CDFs of some random variables. There are a number of well known probability distributions, a few of which will be presented here. Probability distributions can be used to model populations. Let's consider our coin toss example. We did not actually toss thousands of coins to come up with their probability distribution. We modeled the population of coin tosses using their probability distribution. We say that a random variable $X$ *follows* or *has* some distribution. Distributions have parameters and if we do not specify the parameters we usually speak of a *family of distributions*. If $X$ follows the distribution $D$ and $a,\ b$ are its parameters, we write:

$$
X \sim D(a, b)
$$

We usually want to know what outcome we expect on average given a distribution. We can use the concept of an expected value, denoted $\mathbb{E}[X]$. In addition, the variance $\left(Var(X)\right)$ gives a measure of spread around the mean. If the variance is high, values far away from the mean are more likely. Conversely, if variance is low, values far away from the mean are less likely. 

Let's start again with discrete distributions and then move on the continuous ones. 

## Discrete Distributions

For discrete distributions the expected value is defined as the sum of all possible values weighted by their respective probability mass. That is, values that are very unlikely get less weight and those that are very likely get more weight. This can be written as

$$
	\mathbb{E}[X] = \sum_{x} x f_{X}(x) = \sum_x x P(X = x) 
$$

The variance is defines as

$$
Var(X) = \mathbb{E}\left[\left(X - \mathbb{E}[X] \right)^2 \right] = \mathbb{E}[X^{2}] - ( \mathbb{E}[X])^{2}
$$

This is the expected squared deviation from the expected value. Taking the squared deviation always yields a positive value. In addition, larger deviations are emphasized. This is visualized in the plot below. Some observations: The tosses that do not deviate from the mean and those that only deviate by 1 stay the same when squaring. Those that are $-1$ become $+1$ and all others are becoming positive and increase compared to their absolute value.

<iframe src="./sqrerror.html", width="900", height="600"/>


### Binomial Distribution - Coin toss yet again!
It turns out that the random variable "number of heads observed" follows a very common distribution, the **binomial distribution**. This can be written as follows: $X$ being the number of heads observed,

$$
X \sim binomial(n, p)
$$

where $n$ are the number of coins and $p$ is the probability of observing head. Here $n,\ p$ are the parameters of the *binomial* distribution. 


```{r discreteDist}
y <- rbinom(1e5, 5, (1/6))
plot(NULL, xlim=c(0, 5), ylim = c(0,1), 
     ylab = "Cumulative Probability", xlab = "observed #", xaxt = "n", main="Cumulative distribution function of the Binomial Distribution", adj = 0)
mtext("p = 1/6, n = 5", adj = 0)
axis(1, at = 0:5)
grid(NULL, NULL, lwd = 1, lty = 'solid', col = "gray93", equilogs = FALSE)
lines(ecdf(y))
```

<!---
TODO Replace with shiny app 
--->

The binomial distribution can be used whenever there are two or more possible outcomes, one of which is seen as "success", that is we want to add $1$ if this outcome occurs and the other(s) as failure in which case nothing is added. The idea is based on the concept of [Bernoulli trials](https://en.wikipedia.org/wiki/Bernoulli_trial). It works analogously for dice if we are interested in the number of dice the show a particular value, say $1$. 

1. Throw any number of dice, say $5$.
2. For each die check if it shows $1$.
3. If yes add 1, if no, do not add anything.
5. The random variable is the final number and follows a binomial distribution with $p = \frac{1}{6},\ n = 5$

```{r}
library(ggplot2)
y <- data.frame(x=0:5, y=dbinom(0:5, 5, (1/6)))
ggplot(y, aes(x = x, y = y))+
  geom_point()+
  theme_bw()+
  scale_x_continuous(breaks = 0:10, labels=0:10)+
  labs(x = "observed #", y = "Probability Mass", title = "Probability mass function of the Binomial distribution", subtitle = "p = 1/6, n = 5")
```

So, given the parameters $p,\ n$ of the binomial distribution what are the expected value and the variance?

Let's start with the coin toss with a fair coin:
Now $p = 0.5,\ n = 1$ and $X_{0}$ is again the number of heads observed. We sum over all possibilities and weigh by the probability:

$$
0.5 * 1 + 0.5 * 0 = 0.5 = \mathbb{E}[X_{0}]
$$

What happens if we change the probability of observing head to $0.8$?
Then the random variable $X_1$ has expectation

$$ 
0.8 * 1 + 0.2 * 0 = 0.8 = \mathbb{E}[X_{1}]
$$ 

What happens if we change the number of coins to $2$ and keep $p = 0.8$?
Then the random variable $X_2$ has expectation

$$
\underbrace{0.8 * 1 + 0.2 * 0}_{\text{first coin}} + \underbrace{0.8 * 1 + 0.2 * 0}_{\text{second coin}} = 2 * 0.8 = 1.6 = \mathbb{E}[X_{2}]
$$

In general you can just sum up the probability of "success" of all the coins tossed.
If $X\sim binomial(n,\ p)$ then

$$
\mathbb{E}[X] = n * p
$$

for any appropriate $p$ and $n$. 

The variance is the expected squared deviation from the expected value. Let's look at a single toss of a fair coin again ($p = 0.5,\ n = 1$). We already know the expected value is $\mathbb{E}[X_0] = 0.5$. When we toss the coin we could get heads such that $x = 1$ with probability $p = 0.5$ or we could get tails such that $x = 0$ with probability $1-p = 0.5$. In either case we deviate from the expected value by $0.5$. Now we use the definition of the expectation as the weighted sum and the fact that we are interested in the squared deviation

$$
Var(X_0) = 0.5 * (0.5^2) + 0.5 * (0.5^2) = 2 * 0.5 * (0.5^2) = 0.5 - 0.5^2 = 0.25
$$

What happens if we change the probability of observing heads to $0.8$?
Now the expected value is $\mathbb{E}[X_{1}] = 0.8$ and we deviate from it by $0.2$ if we get heads and by $0.8$ if we get tails. We get

$$
Var(X_1) = \underbrace{0.8}_{p(h)} * \underbrace{(0.2^2)}_{deviation} + 0.2 * (0.8^2) = 0.8 - 0.8^2 = 0.16
$$

How did we get to the second to last equality?

Notice that the sum follows this pattern for $n = 1$ and any appropriate $p$:

$$
Var(X_i) = p * (1-p)^2 + (1-p) * p^2
$$

If we expand the squared term and simplify:

\begin{align*}
Var(X_i) &= p * (1 - 2*p + p^2) + p^2 - p^3 \\
&= p - 2*p^2 + p^3 + p^2 - p^3 \\
&= p - p^2
\end{align*}

What happens if we change the number of coins to $2$ and keep $p=0.8$?

$$
Var(X_2) = 0.8 * 0.2^2 + 0.2 * 0.8^2 + 0.8 * 0.2^2 + 0.2 * 0.8^2 = 2 * (0.8 * 0.2^2 + 0.2 * 0.8^2) = 2 * (0.8 - 0.8^2) = 0.32
$$

Since increasing $n$ further simply adds more of the same terms we can easily adapt the general formula above for any appropriate $n$ and $p$:

$$
Var(X_i) = n * (p-p^2)
$$

Equivalently this formula can be written as:

$$
n * (p - p^2) = np - np^2 = np * (1-p) = Var(X_i)
$$

You can directly get values from the binomial distribution in R

```{r}
# Parameters
p <- 0.8
n <- 2
# Random sample with 100 observations
head(rbinom(100, n, p))
# Probability mass at point 2
dbinom(2, n, p)
# Cumulative probability at point 1
pbinom(1, n, p)
# Quantile function at percentiles 0.36 and 0.37
qbinom(0.36, n, p)
qbinom(0.37, n, p)
```

<!--- TODO Make shiny app ---> 

See [Quantile Function](https://en.wikipedia.org/wiki/Quantile_function).

### Discrete Uniform Distribution

The discrete uniform distribution assigns the same probability to all possible values. 

```{r}
dat <- data.frame(x = 1:10, y = rep( ( 1 / 10), time = 10))
ggplot(dat, aes(x = x, y = y)) +
  geom_linerange(aes(ymin = 0, ymax = y)) +
  geom_point() +
  geom_line(aes(x = 1:10, y = rep(0, times = 10))) +
  theme_bw() +
  scale_x_continuous(breaks = c(1:10)) +
  lims(y = c(0, 0.15)) +
  labs(y = "Probability Mass", x = "Event",
       title = "PMF of a Discrete Uniform Distribution")

cdfdat <- data.frame(y = cumsum(rep(0.1, times = 10)), x = 1:10)
ggplot(cdfdat, aes(x = x, y = y)) +
  geom_segment(aes(x = x, xend = (x + 1), yend = y)) +
  geom_point() +
  theme_bw() +
  labs(y = "Cumulative Probability", x = "Event",
       title = "CDF of a Discrete Uniform distribution") +
  scale_x_continuous(breaks = c(0:10))
```

To calculate the expected value of this distribution let's first look at how to easily sum the numbers from $1$ to some arbitrary $N$. That is $1 + 2 + 3 + \dots + N =$ ?. Let $S = 1 + 2 + 3 + \dots + N = \sum_{i = 1}^N i$. Then

\begin{align*}
S &= 1 + 2 + 3 + \dots + (N-2) + (N-1) + N \\
\text{This can be rearranged to:} \\
S &= N + (N-1) + (N-2) + \dots + 3 + 2 + 1 \\
\text{Summing the two yields:} \\
2 * S &= (1 + N) + (2 + N - 1) + (3 + N - 2) + \dots + (N -2 + 3) + (N - 1 + 2) + (N + 1)\\
&= (1 + N) + (1+N) + (1+N) + \dots + (1+N) + (1+N) + (1+N) \\
&= N * (1 + N) = 2 * S \\
\text{It follows that:}\\
S&= \frac{N * (1 + N)}{2}
\end{align*}

he weight given to each possible outcome must be equal and is thus $p = \frac{1}{N}$. 
Recall that the expected value is the weighted sum of all possible outcomes.
Thus if $X \sim discrete\ uniform(N)$
$$
\mathbb{E}[X] = \sum_{i = 1}^N p * i = \sum_{i = 1}^N \frac{1}{N}* i = \frac{1}{N} \sum_{i = 1}^N i = \frac{1}{N} * S = \frac{1}{N} * \frac{N * (1 + N)}{2} = \frac{(1 + N)}{2}
$$

Figuring out the variance is a bit more involved. Since we already know $\mathbb{E}[X]$ we still need $\mathbb{E}[X^{2}]$. Again we apply our equal weight to all the elements and get

$$
\mathbb{E}[X^{2}] = \sum_{x = 1}^n x^2 * \frac{1}{N} = \frac{1}{N} \sum_{x = 1}^N x^2 
$$

Therefore we need to find out what $1 + 4 + 9 + 16 + \dots + N^2$ is equal to. Luckily [there exists a formula for that](https://www.khanacademy.org/math/calculus-home/series-calc/series-basics-challenge/v/sum-n-squares-2):

$$
\sum_{x=1}^N x^2 = \frac{N * (N + 1) * (2*N + 1)}{6}
$$

Thus,

$$
\mathbb{E}[X^{2}] = \frac{(N + 1) * (2*N + 1)}{6}
$$

and

$$
Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \frac{(N + 1) * (2*N + 1)}{6} - \left(\frac{(1 + N)}{2}\right)^{2} = \frac{(N+1) * (N-1)}{12}
$$

## Continuous Distributions

A distribution is continuous if the cumulative distribution function is a continuous function (no steps!). 

```{r, }
library(cowplot)

cdf <- ggplot(data.frame(x = c(-5, 5)), aes(x)) +
  stat_function(fun = punif, args = list(min = -5, max = 5), geom = "line", aes(color = "Uniform")) +
  stat_function(fun = pnorm, geom = "line", aes(color = "Normal")) + 
  labs(x = "Value", y = "Cumulative Probability", title = "CDF of continuous distributions") +
  theme_bw() +
  theme(legend.title=element_blank(), legend.pos = "bottom")
pdf <- ggplot(data.frame(x = c(-5, 5)), aes(x)) +
  stat_function(fun = dunif, args = list(min = -5, max = 5), geom = "line", aes(color = "Uniform")) +
  stat_function(fun = dnorm, geom = "line", aes(color = "Normal")) + 
  labs(x = "Value", y = "Probability density", title = "PDF of  continuous distributions") +
  theme_bw() +
  theme(legend.title=element_blank(), legend.pos="bottom")
plot_grid(cdf, pdf, align = c(axis = "b"), rel_widths = 1) 
```

As a consequence we cannot simply sum up values to get an expected value or a variance. We are now dealing with real numbers and thus there are infinitely many values between any two arbitrary numbers that are not equal.

Therefore, instead of the sum we have to evaluate the integral of all the possible values weighted by the probability density function (the continuous equivalent to the probability mass function). 

$$
\mathbb{E}[X] = \int_{-∞}^{∞} x f_{X}(x) dx
$$

where $f_X(x)$ is the density of the random variable $X$ evaluated at some point $x$ and the integral over $x$ ("$dx$") has the same purpose as the sum over $x$ before. 

### Uniform Distribution

To illustrate the concept of the integral the uniform distribution provides a simple example. As in the continuous case it assigns equal weight to each equally sized interval in the area on which the variable is defined ($[a, b]$). Why each interval and not each value? Since there are infinitely many values between $a$ and $b$ (again due to real numbers) each individual value cannot be assigned a probability small enough for all of the probabilities to sum to $1$ (which is a basic requirement of a probability). Thus we can only assign a probability to an interval, e.g. $[0, 0.001]$, of which only *finitely* many exist between $a$ and $b$, e.g. $a = -2$ and $b = 1$. In this example there exist $3,000$ intervals of values $[x, x + 0.001]$. Since we are dealing with intervals the probability density can be thought of as the area under the PDF for a given interval or the sum of the areas of *very small* intervals within the chosen interval. 

<!--- TODO here dist_unif shiny app --->

The PDF is defined as

$$
f_X(x) = \frac{1}{b-a} \text{ if } x \in [a, b], \ 0 \text{ otherwise}
$$

That is, the weight $\frac{1}{b-a}$ is assign to values in the interval of interest and all other values have weight $0$. As already mentioned *all* values between $a$ and $b$ have to be considered. Thus, in order to calculate the expected value and the variance we have to integrate over $x$.

$$
\mathbb{E}[X] = ∫_a^b x * \frac{1}{b-a} dx = \frac{b+a}{2}
$$

If you plug in $a = 1$ in the formula above you can see the relation to the discrete uniform distribution and the similar role of integral and summation. Notice also how the expectation operator "translates" to the integral. For the expectation of $X$ we integrate over all $x$, the possible realizations of $X$, weighted by the PDF of $X$. Now, in order to get the variance we want to calculate the expected squared deviation from the expected value.

\begin{align*}
Var(X) &= \mathbb{E}\left[(X - \mathbb{E}[X])^{2} \right] = \mathbb{E}\left[\left(X - \frac{b+a}{2}\right)^2\right] \\
&= ∫_a^b \left(x - \frac{b+a}{2}\right)^2 * \frac{1}{b-a} dx = \frac{(b-a)^2}{12}
\end{align*}

Clearly the Uniform distribution can be used whenever we want to model a population in which all possible outcomes are equally likely. 

### Normal distribution

The normal distribution is probably the most widely known one. Its PDF is the famous bell curve. It has two parameters $\mu$, and $\sigma^2$. $\mu$ is the mean and $\sigma^2$ the variance of the distribution. In the case of $\mu = 0,\ \sigma^2 = 1$ it is called the *standard normal distribution*.

The Normal distribution has a few nice properties. It is symmetric around the mean which is nice whenever we want to express the believe that values are less likely the further we get away from the mean but we do not care in which direction. In addition, it can be used to approximate many other distributions including the Binomial distribution under certain conditions (see [Central Limit Theorem](#central-limit-theorem)). The normal distribution can be *standardized*, i.e. given any random normal variable, $X\sim N(\mu, \sigma^2)$, we can get a *standard normal* variable $Y \sim N(0, 1)$ where $Y = \frac{X - \mu}{\sigma}$. This means that we can perform calculations using the standard normal distribution and then recover the results for any normal distribution since for a standard normal $Y\sim N(0,1)$ we get that $X \sim N(\mu, \sigma^{2}$ where $X = \mu + \sigma * Y$ by just rearranging the formula above. 


